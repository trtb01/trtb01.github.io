<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Facial Expression AI Analyzer</title>
    
    <!-- Load Google Font 'Inter' --><link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;700&display=swap" rel="stylesheet">

    <style>
        /* --- 1. Color and Font Variables --- */
        :root {
            --color-black: #000000;
            --color-white: #FFFFFF;
            --color-red: #FF0000;
            
            --font-main: 'Inter', sans-serif;
        }

        /* --- 2. Base & Layout Styling --- */
        body {
            background-color: var(--color-black);
            color: var(--color-white);
            font-family: var(--font-main);
            display: flex;
            flex-direction: column;
            align-items: center;
            margin: 0;
            padding: 2rem;
            min-height: 100vh;
        }

        h1 {
            color: var(--color-white);
            text-align: center;
            font-weight: 700;
            letter-spacing: 1px;
            margin-bottom: 2rem;
        }

        /* --- 3. Display Container --- */
        .container {
            display: flex;
            justify-content: center;
            gap: 2rem;
            width: 100%;
            max-width: 1400px;
            /* Allow wrapping on smaller screens */
            flex-wrap: wrap;
        }

        .display-box {
            border: 4px solid var(--color-red);
            overflow: hidden;
            background-color: #111; /* Dark bg for loading */
            flex-basis: 640px; /* Default size */
            flex-grow: 1;
            max-width: 720px;
            aspect-ratio: 16 / 9;
            display: flex;
            justify-content: center;
            align-items: center;
            /* Position relative for drawing text on top */
            position: relative; 
        }

        #webcam, #wireframeCanvas {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }
        
        #wireframeCanvas {
            position: absolute;
            top: 0;
            left: 0;
        }

        #webcam {
            /* Flip the webcam feed horizontally for a "mirror" effect */
            transform: scaleX(-1);
        }

        /* --- 4. Explanation Box --- */
        .explanation {
            border: 2px solid var(--color-white);
            padding: 1.5rem 2rem;
            margin-top: 2rem;
            width: 100%;
            max-width: 1400px;
            box-sizing: border-box; /* Include padding/border in width */
        }

        .explanation h2 {
            color: var(--color-red);
            margin-top: 0;
            border-bottom: 1px solid #555;
            padding-bottom: 0.5rem;
        }

        .explanation ul {
            padding-left: 20px;
            list-style-type: disc;
        }

        .explanation li {
            margin-bottom: 0.75rem;
            line-height: 1.6;
        }

        /* --- 5. Loading Message & Controls --- */
        #startButton {
            background-color: var(--color-black);
            color: var(--color-white);
            border: 2px solid var(--color-red);
            padding: 0.75rem 1.5rem;
            font-family: var(--font-main);
            font-size: 1rem;
            font-weight: 700;
            cursor: pointer;
            margin-bottom: 1rem;
            transition: all 0.2s;
        }
        
        #startButton:hover {
            background-color: var(--color-red);
            color: var(--color-black);
        }

        #startButton:disabled {
            background-color: #333;
            border-color: #555;
            color: #888;
            cursor: not-allowed;
        }

        #loadingMessage {
            position: fixed;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            font-size: 1.2rem;
            background-color: rgba(0, 0, 0, 0.9);
            padding: 1.5rem 2rem;
            border: 1px solid var(--color-red);
            display: none; /* Hidden by default */
            z-index: 100;
            text-align: center;
        }
        
        /* --- 6. Live Console --- */
        #liveConsole {
            border: 2px solid var(--color-white);
            background-color: #111;
            width: 100%;
            max-width: 1400px;
            box-sizing: border-box;
            height: 150px;
            margin-top: 2rem;
            overflow-y: scroll;
            padding: 1rem;
            font-family: 'Courier New', Courier, monospace;
        }
        
        #liveConsole p {
            margin: 0 0 0.5rem 0;
            color: #ccc;
        }
        
        #liveConsole p:last-child {
            margin-bottom: 0;
        }
        
        #liveConsole .log-error {
            color: var(--color-red);
            font-weight: 700;
        }
        
        #liveConsole .log-success {
            color: #00ff00; /* Green for success */
        }
        
    </style>
</head>
<body>

    <h1>Real-time Facial Expression AI Analyzer :3</h1>
    
    <!-- Controls --><button id="startButton">Start Webcam</button>

    <!-- Loading message, hidden by default --><div id="loadingMessage">Loading AI Model & Accessing Webcam...</div>

    <!-- Main container for video and canvas outputs --><div class="container">
        <div class="display-box">
            <video id="webcam" autoplay playsinline></video>
        </div>
        <div class="display-box">
            <!-- The canvas will be drawn over with mesh and expression data --><canvas id="wireframeCanvas"></canvas>
        </div>
    </div>

    <!-- Live Console Feed --><div id="liveConsole">
        <p>Waiting for user to start webcam...</p>
    </div>

    <!-- Explanation section below the displays --><div class="explanation">
        <h2>How It Works</h2>
        <ul>
            <li><strong>Webcam Access:</strong> Click the button to request webcam access via your browser's MediaDevices API.</li>
            <li><strong>AI Model:</strong> This page loads Google's MediaPipe FaceLandmarker, a model that runs directly in your browser.</li>
            <li><strong>Face Mesh Detection:</strong> The model analyzes the video stream to identify 478 unique 3D landmarks (key points) on your face.</li>
            <li><strong>Expression Analysis:</strong> It also analyzes "blendshapes" to quantify facial expressions (e.g., "mouthSmile", "jawOpen"). The top 5 non-squint expressions are shown on the wireframe.</li>
            <li><strong>Wireframe Drawing:</strong> The coordinates of the 478 points are connected to draw the live face mesh, which is overlaid on a black background.</li>
            <li><strong>Local only!</strong>This file is fully local. Feel free to download it and run it off of your own system.</li>
        </ul>
    </div>


    <script type="module">
        // --- 1. Import necessary MediaPipe modules ---
        import {
            FaceLandmarker,
            FilesetResolver,
            DrawingUtils // We'll use this to help draw
        } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/vision_bundle.js";

        // --- 2. Get DOM Elements ---
        const video = document.getElementById("webcam");
        const canvas = document.getElementById("wireframeCanvas");
        const ctx = canvas.getContext("2d");
        const loadingMessage = document.getElementById("loadingMessage");
        const startButton = document.getElementById("startButton");
        const consoleDiv = document.getElementById("liveConsole");
        
        // --- 3. Global Variables ---
        let faceLandmarker;
        let lastVideoTime = -1;
        // Helper for drawing MediaPipe results
        let drawingUtils; 

        // --- 4. Color Constants (from CSS) ---
        const COLOR_LANDMARK = "#FF0000"; // Red
        const COLOR_CONNECTOR = "#FFFFFF"; // White
        const COLOR_TEXT = "#FF0000"; // Red
        
        // --- 5. Live Console Logging Function ---
        function logToConsole(message, type = 'log') {
            console.log(message); // Also log to browser dev console
            
            const p = document.createElement('p');
            p.textContent = `[${new Date().toLocaleTimeString()}] ${message}`;
            
            if (type === 'error') {
                p.className = 'log-error';
            } else if (type === 'success') {
                p.className = 'log-success';
            }

            consoleDiv.appendChild(p);
            consoleDiv.scrollTop = consoleDiv.scrollHeight; // Auto-scroll to bottom
        }

        // --- 6. Add Start Button Event Listener ---
        startButton.addEventListener('click', initializeApp);

        async function initializeApp() {
            startButton.disabled = true;
            
            try {
                logToConsole("Loading AI model...");
                loadingMessage.innerText = "Loading AI model (this may take a moment)...";
                loadingMessage.style.display = 'block';
                
                await setupFaceLandmarker();
                
                logToConsole("AI model loaded successfully.", 'success');
            } catch (error) {
                const errorMsg = error.message || "An unknown error occurred.";
                logToConsole(`Failed to load AI model: ${errorMsg}`, 'error');
                logToConsole("This is likely due to a browser extension (ad-blocker) or network issue.", 'error');
                loadingMessage.innerText = `Error: ${errorMsg}. Please check console and refresh.`;
                startButton.disabled = false;
                return;
            }

            try {
                loadingMessage.innerText = "Requesting webcam access...";
                logToConsole("Requesting webcam access...");

                await setupWebcam();
                logToConsole("Webcam access granted. Video feed started.", 'success');

                loadingMessage.style.display = 'none';
                logToConsole("Starting real-time detection...");
                predictWebcam();

            } catch (error) {
                const errorMsg = error.message || "An unknown error occurred.";
                logToConsole(`Initialization failed: ${errorMsg}`, 'error');
                loadingMessage.innerText = `Error: ${errorMsg}. Please check console and refresh.`;
                startButton.disabled = false;
            }
        }
        
        // --- 7. Initialize and Run Face Landmarker ---
        async function setupFaceLandmarker() {
            try {
                logToConsole("Loading vision task WASM files...");
                const vision = await FilesetResolver.forVisionTasks(
                    "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm"
                );
                logToConsole("WASM files loaded.");

                // Initialize the drawing helper
                drawingUtils = new DrawingUtils(ctx);

                logToConsole("Creating FaceLandmarker instance...");
                faceLandmarker = await FaceLandmarker.createFromOptions(vision, {
                    baseOptions: {
                        modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task`,
                        delegate: "GPU",
                    },
                    runningMode: "VIDEO",
                    outputFaceBlendshapes: true, // This is key for expression data
                    outputFacialTransformationMatrixes: true,
                    numFaces: 1, // Track one face for performance
                });
                logToConsole("FaceLandmarker instance created.");

            } catch (error) {
                console.error("Error setting up FaceLandmarker:", error);
                throw new Error("Failed to load AI model. Check browser console for details.");
            }
        }
        
        // --- 8. Setup Webcam ---
        async function setupWebcam() {
            if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                throw new Error("Webcam access (getUserMedia) is not supported by your browser.");
            }

            return new Promise(async (resolve, reject) => {
                try {
                    const stream = await navigator.mediaDevices.getUserMedia({
                        video: { width: 1280, height: 720 }
                    });
                    
                    video.srcObject = stream;
                    
                    video.addEventListener("loadeddata", () => {
                        logToConsole("Webcam 'loadeddata' event fired.");
                        
                        // Set canvas dimensions to match the video
                        canvas.width = video.videoWidth;
                        canvas.height = video.videoHeight;
                        logToConsole(`Canvas resized to ${canvas.width}x${canvas.height}`);
                        
                        resolve();
                    });

                    video.addEventListener("error", (e) => {
                        reject(new Error("A video error occurred."));
                    });

                } catch (error) {
                    console.error("Error accessing webcam:", error);
                    reject(new Error("Webcam access denied. Please grant permission and refresh."));
                }
            });
        }

        // --- 9. Prediction Loop ---
        async function predictWebcam() {
            if (video.currentTime !== lastVideoTime && faceLandmarker) {
                lastVideoTime = video.currentTime;
                const startTimeMs = performance.now();
                
                // Detect face landmarks
                const results = faceLandmarker.detectForVideo(video, startTimeMs);

                // Clear the canvas
                ctx.clearRect(0, 0, canvas.width, canvas.height);

                // --- Draw the results ---
                if (results.faceLandmarks && results.faceLandmarks.length > 0) {
                    
                    // --- Draw the Face Mesh ---
                    // The canvas needs to be flipped horizontally to match the mirrored video
                    ctx.save();
                    ctx.scale(-1, 1);
                    ctx.translate(-canvas.width, 0);
                    
                    const landmarks = results.faceLandmarks[0]; // Get the first face
                    
                    // Draw the mesh
                    drawingUtils.drawConnectors(
                        landmarks,
                        FaceLandmarker.FACE_LANDMARKS_TESSELATION,
                        { color: COLOR_CONNECTOR, lineWidth: 0.5 }
                    );
                    
                    // Draw the key points (dots)
                    // We'll draw only a few key points, not all 478, to avoid clutter
                    // Let's draw the outline of the lips
                    drawingUtils.drawConnectors(
                        landmarks,
                        FaceLandmarker.FACE_LANDMARKS_LIPS,
                        { color: COLOR_LANDMARK, lineWidth: 2 }
                    );
                    // And the eyes
                    drawingUtils.drawConnectors(
                        landmarks,
                        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,
                        { color: COLOR_LANDMARK, lineWidth: 2 }
                    );
                     drawingUtils.drawConnectors(
                        landmarks,
                        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,
                        { color: COLOR_LANDMARK, lineWidth: 2 }
                    );
                    // And the eyebrows
                     drawingUtils.drawConnectors(
                        landmarks,
                        FaceLandmarker.FACE_LANDMARKS_LEFT_EYEBROW,
                        { color: COLOR_LANDMARK, lineWidth: 2 }
                    );
                     drawingUtils.drawConnectors(
                        landmarks,
                        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYEBROW,
                        { color: COLOR_LANDMARK, lineWidth: 2 }
                    );

                    ctx.restore(); // Restore context before drawing text
                    
                    
                    // --- Draw the Expression Data ---
                    if (results.faceBlendshapes && results.faceBlendshapes.length > 0) {
                        drawExpressionData(results.faceBlendshapes[0]);
                    }
                }
            }

            // Call this function again on the next animation frame
            requestAnimationFrame(predictWebcam);
        }

        // --- 10. Drawing Functions ---

        function drawExpressionData(blendshapes) {
            // Define the blendshapes to ignore
            const ignoredBlendshapes = ["eyeSquintLeft", "eyeSquintRight"];

            // Filter out ignored blendshapes, then filter for significant scores, then sort and slice
            const topExpressions = blendshapes.categories
                .filter(category => !ignoredBlendshapes.includes(category.categoryName)) // Ignore squints
                .map(category => ({
                    name: category.categoryName,
                    score: category.score
                }))
                .filter(expression => expression.score > 0.5) // Only show significant expressions (CHANGED from 0.2 to 0.5)
                .sort((a, b) => b.score - a.score) // Sort descending
                .slice(0, 5); // Get top 5

            // Set text style
            ctx.fillStyle = COLOR_TEXT;
            ctx.font = "bold 24px Inter";
            ctx.shadowColor = "black";
            ctx.shadowBlur = 4;

            // Draw each expression
            topExpressions.forEach((expression, index) => {
                const text = `${expression.name}: ${(expression.score * 100).toFixed(1)}%`;
                ctx.fillText(text, 20, 40 + index * 30);
            });
            
            ctx.shadowBlur = 0; // Reset shadow
        }
        
        // No automatic start, app waits for button click.
        logToConsole("App initialized. Waiting for user to click 'Start Webcam'.");

    </script>
</body>
</html>